<!doctype html>
<html lang="en">

<head>
  <link rel="icon" href="../images/aiqfome.svg" />
  <bm-article-head title="seq2seq"></bm-article-head>
  <script src="../script/articleHead.js"></script>
  <script src="../script/nagvigator.js"></script>
</head>

<body>
  <bm-navigator></bm-navigator>
  <bm-article title="seq2seq">
    本文只是一篇学习笔记，学习材料链接已在文章最后给出。

按笔者的习惯，首先来进行名词解释，<strong>什么是 seq2seq</strong>，之后所有新名词(于笔者而言)都会先进行一次名词解释。

Seq2seq 即为 Sequence to Sequence ，是一种用于自然语言处理的机器学习方法，顾名思义，这种方法能将一个「(词)序列」转化为另一个「(词)序列」。
可能的应用场景有：翻译、图像描述、对话模型和文本总结。其中最经典的，也是最早的应用场景便是翻译。

但笔者认为就像名字上没有提到语言，而只是对输入输出形式化的描述，这种思想其实能对任意「序列」形式的内容进行使用。

Seq2seq 模型由两部分组成：1. Encoder 2. Decoder，而这两部分都通常是用 各种 RNN 来实现的，前者获得不定长的输入文本，将其转化为固定长度的向量后输入给后者，后者根据该向量得到我想要的输出，不定长的文本序列

Encoder 在处理输入的过程中需要捕获输入的重要信息(特征)，并将其存储为网络的隐状态(hidden state) (特别的，如果模型带有注意力机制，还会输出一个 context vetcor，该向量是输入隐状态的加权和，为输出序列的每个时间实例生成）

Decoder 借助 隐状态和 context vector生成最终输出，并且采用自回归的方式，一次只生成输出序列中的一个元素。每一步生成都会考虑先前生成的元素、context vector 和输入序列的信息。具体来说，在具有注意力机制的模型中，context vector 和隐状态连接在一起形成注意力隐向量，该向量用作解码器的输入

自回归：autoregressive(AR), 把上一步的结果作为下一步的输入(GPT 采用的就是这个方法)，好吧，更精确的解释是，输出变量线性依赖于自身先前的值和随机项(不完全可预测的项)，也可以说是先前状态的函数

更具体的解释等笔者哪天对数学感兴趣了再说吧），虽然有没有下一篇文章都难说

Attention 机制是由 Bahdanau(总觉得这名字是个印度人，但是加拿大) 引入的，用于解决朴素 Seq2seq 结构中，长输入中隐状态中信息失真的情况。允许在解码过程中有选择性地关注输入的不同部分，解码器每一步 alignment model 都采用当前状态和“所有” 注意力隐向量来计算 attention score，配合 softmax 就可以得到注意力权重

alignment model 是一个额外的 NN model 与 Seq2seq 联合训练，用来计算  由隐状态表示的输入 与 由注意力隐状态表示的先前的输出之间的匹配程度

这样好像就结束了……肯定不能这样，还有许多细节还未探讨，究竟要如何通过机器学习来实现这个 Seq2seq 模型？

因为翻译是其最早的应用(其实是阅读的文章里的应用场景全是翻译)，所以我们也用翻译来举例。

一个非常经典的案例是 NMT(Neural Machine Translation)

NMT：

SMT：

这里笔者主要叙述(翻译摘录)一篇大名鼎鼎的 Ilya 在 2014 年发表的一篇文章 《Sequence to Sequence Learning with Neural Networks》

文章首先论述了 DNN 在语音识别和视觉对象识别上取得了非凡的表现。其强大源于 它能在适当步骤下进行任意平行计算（？没看懂）。尽管 NN 与传统的统计模型相关，他们能学会复杂的计算。

所以，如果一个存在一个参数设置使得大型的 DNN 能得到良好的结果，那么有监督的反向传播将会找到这些参数并解决这个问题。

尽管 DNN 具有强大的能力和灵活性，它只能处理那些被能被合理编码为固定维度向量的问题。而许多重要问题的序列长度都无法提前得知，并且是 map Sequence to Sequence 的。

why？

由此引出文章的目的，一个能学习将序列映射到序列的领域独立的方法将会非常有用。

本文展示了一个用 LTSM(Long Short-Term Memory) 结构解决一般的 seq2seq 问题的直接应用

LSTM：

使用一个 LSTM 读取输入序列(一次一个 timestep) 输出大的固定维度的向量表示，接着 用 另一个 LSTM 从向量中解析出输出序列

第二个 LSTM 本质上是一个 RNN 除了以输入序列为条件

为什么需要两个 LSTM？→

因为 对于RNN 而言 只要提前得知输入输出的对齐情况，RNN就可以轻松完成映射，但(文章完成的时候)还不清楚如何将 RNN 应用于 输入输出 长度不同并且有复杂而非单调关系(就是输入输出均可变长)

最简单的策略就是，使用两个 RNN ，一个将不定长输入映射到固定维度向量，另一个将向量映射到目标序列

其实 RNN 可以输出不定长的数据

为什么需要一个固定维度的向量作为中介？→因为要获取一遍所有特征？然后固定维度比较简单？

选择 LSTM 是因为 其能够成功学习具有长范围时间依赖性的数据(解决 RNN 获得全部相关信息后会产生长期依赖性而难以训练的问题)，而该场景下输入和输出之间存在相当大的时间滞后

LSTM 的一个有用特性是它能学会将一个可变长的输入句子编程一个固定维度的向量表示

LSTM 是用来估计输入到输出的条件概率的 $p(y_1,…,y_{T’} | x_1,…,x_T)$

计算条件概率的方法是：1. 获取由最后一个 隐状态给出的 (x_1,…,x_T) 的固定维度表示 v。 2. 采用标准的 LSTM-LM 公式计算 (y_1,…,y_{T’}) 的概率(初始隐状态是 v)

还使用了 <EOS> 表示句子结尾，这样使得模型能够定义所有可能长度的序列上的分布

使用两个不同的LSTM 可以以很少的计算成本增加模型参数的数量，并且自然地同时在多个语言对上训练 LSTM

由于深度 LSTM 的性能明显优于浅层 LSTM 所以使用了四层 LSTM

发现通过简单的倒转输入，可以使得 SGD 更容易地在输入和输出之间建立通信，极大地提高 LSTM 的性能

使用简单的从左到右 beam search decoder 来搜索最佳结果





    1. https://s3tlxskbq3.feishu.cn/docx/NyPqdCKraoXz9gxNVCfcIFdnnAc#ZWSwdmWQaoGAQuxkOEHcuD5UnVl
    2. https://arxiv.org/pdf/1409.3215.pdf
    3. https://en.wikipedia.org/wiki/Autoregressive_model
    4. https://en.wikipedia.org/wiki/Seq2seq#:~:text=Seq2seq%20is%20a%20family%20of,one%20sequence%20into%20another%20sequence.
  </bm-article>
  <script src="../script/controlPixelRatio.js"></script>
  <script src="../script/articleTemplate.js"></script>
  <script src="../script/articleImage.js"></script>
</body>

</html>
